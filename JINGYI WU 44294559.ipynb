{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://8762a380f160:4041\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1602068997506)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = shakespeare.txt MapPartitionsRDD[1] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\n",
      "\n",
      "THE SONNETS\n",
      "\n",
      "by William Shakespeare\n",
      "\n",
      "\n",
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n"
     ]
    }
   ],
   "source": [
    "lines.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\n",
      "THE SONNETS\n",
      "by William Shakespeare\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at <console>:26\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = lines.filter(x => x.length > 0)\n",
    "rdd1.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\n",
      "THE SONNETS\n",
      "by William Shakespeare\n",
      "                     1\n",
      "  From fairest creatures we desire increase\n",
      "  That thereby beautys rose might never die\n",
      "  But as the riper should by time decease\n",
      "  His tender heir might bear his memory\n",
      "  But thou contracted to thine own bright eyes\n",
      "  Feedst thy lights flame with selfsubstantial fuel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at map at <console>:26\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd2 = rdd1.map(y => y.replaceAll(\"\"\"[\\p{Punct}]\"\"\",\"\"))\n",
    "rdd2.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\n",
      "THE\n",
      "SONNETS\n",
      "by\n",
      "William\n",
      "Shakespeare\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd3: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at flatMap at <console>:26\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd3 = rdd2.flatMap(line => line.split(\" \"))\n",
    "rdd3.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\n",
      "THE\n",
      "SONNETS\n",
      "by\n",
      "William\n",
      "Shakespeare\n",
      "1\n",
      "From\n",
      "fairest\n",
      "creatures\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd4: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at filter at <console>:26\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd4 = rdd3.filter(_.nonEmpty)\n",
    "rdd4.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\n",
      "the\n",
      "sonnets\n",
      "by\n",
      "william\n",
      "shakespeare\n",
      "1\n",
      "from\n",
      "fairest\n",
      "creatures\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd5: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[6] at map at <console>:26\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd5 = rdd4.map( w => w.toLowerCase())\n",
    "rdd5.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abash\n",
      "abashed\n",
      "abashed\n",
      "abashes\n",
      "abashing\n",
      "abate\n",
      "abated\n",
      "abated\n",
      "abates\n",
      "abating\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "allverbs: Array[String] = Array(abash, abashed, abashed, abashes, abashing, abate, abated, abated, abates, abating, abide, abode, abode, abides, abiding, absorb, absorbed, absorbed, absorbs, absorbing, accept, accepted, accepted, accepts, accepting, accompany, accompanied, accompanied, accompanies, accompanying, ache, ached, ached, aches, aching, achieve, achieved, achieved, achieves, achieving, acquire, acquired, acquired, acquires, acquiring, act, acted, acted, acts, acting, add, added, added, adds, adding, address, addressed, addressed, addresses, addressing, adjust, adjusted, adjusted, adjusts, adjusting, admire, admired, admired, admires, admiring, admit, admitted, admitted, admits, admitting, advise, advised, advised, advises, advising, afford, afforded, afforded, affords, affordi...\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val allverbs = sc.textFile(\"all_verbs.txt\").collect()\n",
    "allverbs.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desire\n",
      "increase\n",
      "rose\n",
      "die\n",
      "bear\n",
      "contracted\n",
      "own\n",
      "eyes\n",
      "lights\n",
      "making\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd6: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[9] at filter at <console>:28\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd6 = rdd5.filter(z => allverbs.contains(z))\n",
    "rdd6.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(desire,1)\n",
      "(increase,1)\n",
      "(rose,1)\n",
      "(die,1)\n",
      "(bear,1)\n",
      "(contracted,1)\n",
      "(own,1)\n",
      "(eyes,1)\n",
      "(lights,1)\n",
      "(making,1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd7: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[10] at map at <console>:26\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd7 = rdd6.map(e => (e,1))\n",
    "rdd7.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(float,2)\n",
      "(agree,20)\n",
      "(healing,2)\n",
      "(shot,45)\n",
      "(guide,24)\n",
      "(opening,11)\n",
      "(urging,9)\n",
      "(practises,1)\n",
      "(surge,9)\n",
      "(maintained,2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd8: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[11] at reduceByKey at <console>:26\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd8 = rdd7.reduceByKey((a,b) => (a+b))\n",
    "rdd8.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(abate,14)\n",
      "(abated,3)\n",
      "(abates,1)\n",
      "(abide,37)\n",
      "(abides,8)\n",
      "(abode,9)\n",
      "(accept,30)\n",
      "(accepted,3)\n",
      "(accepts,2)\n",
      "(accompanied,9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd9: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[14] at sortByKey at <console>:26\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd9 = rdd8.sortByKey()\n",
    "rdd9.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(cleave,13)\n",
      "(float,5)\n",
      "(engrave,1)\n",
      "(justify,11)\n",
      "(offer,121)\n",
      "(agree,45)\n",
      "(guide,41)\n",
      "(partake,9)\n",
      "(improve,1)\n",
      "(have,7848)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "verbdict: org.apache.spark.rdd.RDD[String] = verb_dict.txt MapPartitionsRDD[16] at textFile at <console>:27\n",
       "rdd10: org.apache.spark.rdd.RDD[((String, Int), String)] = CartesianRDD[17] at cartesian at <console>:28\n",
       "rdd11: org.apache.spark.rdd.RDD[((String, Int), String)] = MapPartitionsRDD[18] at filter at <console>:29\n",
       "rdd12: org.apache.spark.rdd.RDD[((String, Int), String)] = ShuffledRDD[19] at reduceByKey at <console>:30\n",
       "rdd13: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[20] at flatMap at <console>:31\n",
       "rdd14: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[21] at map at <console>:32\n",
       "rdd15: org.apache.spark.rdd.RDD[(String, Int)] = ZippedPartitionsRDD2[22] at zip at <console>:33\n",
       "rdd16: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[23] at reduceByKey at <console>:34\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val verbdict = sc.textFile(\"verb_dict.txt\")\n",
    "val rdd10 = rdd9.cartesian(verbdict)\n",
    "val rdd11 = rdd10.filter(z => z._2.split(\",\").contains(z._1._1))\n",
    "val rdd12 = rdd11.reduceByKey((a,b) => (a))\n",
    "val rdd13 = rdd12.flatMap(a => a._2.split(\",\").slice(0,1))\n",
    "val rdd14 = rdd12.map(z => z._1._2)\n",
    "val rdd15 = rdd13.zip(rdd14)\n",
    "val rdd16 = rdd15.reduceByKey((a,b) => (a+b))\n",
    "rdd16.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(be,10493)\n",
      "(have,7848)\n",
      "(do,6416)\n",
      "(come,3610)\n",
      "(make,2892)\n",
      "(go,2575)\n",
      "(love,2501)\n",
      "(let,2384)\n",
      "(say,2356)\n",
      "(know,2251)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd17: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[28] at sortBy at <console>:26\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd17 = rdd16.sortBy(_._2,false)\n",
    "rdd17.take(10).foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
